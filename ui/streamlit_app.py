from __future__ import annotations

import io
import os
import sys
from typing import List, Optional, Tuple

import numpy as np
import pandas as pd
import streamlit as st
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline

# è®“ `ml` å¥—ä»¶å¯è¢«åŒ¯å…¥ï¼ˆä»¥å°ˆæ¡ˆæ ¹ç›®éŒ„ç‚ºåŸºæº–ï¼‰
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
if ROOT_DIR not in sys.path:
    sys.path.insert(0, ROOT_DIR)

from ml.data import clean_text, normalize_label  # noqa: E402
from ml.features import make_vectorizer  # noqa: E402
from ml.models import get_model  # noqa: E402

try:
    from ml.metrics import compute_basic_metrics, threshold_sweep  # noqa: E402
except Exception:  # ç›¸å®¹è™•ç†ï¼šè‹¥ç†±é‡è¼‰å°šæœªè¼‰åˆ°æ–°ç‰ˆå‡½å¼
    from ml.metrics import compute_basic_metrics  # type: ignore
    import numpy as _np
    from sklearn.metrics import confusion_matrix as _cm
    import pandas as _pd

    def threshold_sweep(y_true, y_score, thresholds=None):  # type: ignore
        y_true = _np.asarray(y_true)
        y_score = _np.asarray(y_score)
        if thresholds is None:
            lo, hi = float(_np.min(y_score)), float(_np.max(y_score))
            if not _np.isfinite(lo) or not _np.isfinite(hi) or lo == hi:
                thresholds = _np.linspace(0.0, 1.0, 51)
            else:
                thresholds = _np.linspace(lo, hi, 51)
        rows = []
        for t in thresholds:
            y_pred = (y_score >= t).astype(int)
            cm = _cm(y_true, y_pred, labels=[0, 1])
            tn, fp, fn, tp = cm.ravel()
            prec = 0.0 if (tp + fp) == 0 else tp / (tp + fp)
            rec = 0.0 if (tp + fn) == 0 else tp / (tp + fn)
            f1 = 0.0 if (prec + rec) == 0 else 2 * prec * rec / (prec + rec)
            tpr = rec
            fpr = 0.0 if (fp + tn) == 0 else fp / (fp + tn)
            rows.append(
                {
                    "threshold": float(t),
                    "precision": float(prec),
                    "recall": float(rec),
                    "f1": float(f1),
                    "tpr": float(tpr),
                    "fpr": float(fpr),
                }
            )
        return _pd.DataFrame(rows)


st.set_page_config(page_title="Spam Classification Demo", page_icon="ğŸ“¨", layout="centered")


def parse_csv(file_bytes: bytes, no_header: bool) -> pd.DataFrame:
    if no_header:
        df = pd.read_csv(io.BytesIO(file_bytes), header=None, names=["label", "text"], encoding_errors="ignore")
    else:
        df = pd.read_csv(io.BytesIO(file_bytes), encoding_errors="ignore")
        cols = [c.strip().lower() for c in df.columns]
        df.columns = cols
        label_col = "label" if "label" in cols else ("category" if "category" in cols else "target")
        text_col = "text" if "text" in cols else ("message" if "message" in cols else "sms")
        df = df[[label_col, text_col]].copy()
        df.columns = ["label", "text"]
    df["label"] = df["label"].map(normalize_label)
    df["text"] = df["text"].astype(str).map(clean_text)
    return df


def build_vectorizer(vec_kind: str, ngram: Tuple[int, int], token_pattern: str, stop_words_opt: Optional[str]):
    # å„ªå…ˆä½¿ç”¨å°ˆæ¡ˆå°è£ï¼›å¦‚ç°½åä¸åŒå‰‡å›é€€åˆ° sklearn ç›´æ¥å»ºç«‹
    try:
        return make_vectorizer(kind=vec_kind, ngram=ngram, token_pattern=token_pattern, stop_words=stop_words_opt)
    except TypeError:  # èˆŠç‰ˆç°½åç›¸å®¹
        from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

        common = dict(ngram_range=ngram, token_pattern=token_pattern, stop_words=stop_words_opt, lowercase=False)
        return CountVectorizer(**common) if vec_kind == "bow" else TfidfVectorizer(**common)


def list_existing_csvs() -> List[str]:
    candidates: List[str] = []
    for root in ["dataset", "uploads", "data"]:
        if os.path.isdir(root):
            for name in os.listdir(root):
                if name.lower().endswith(".csv"):
                    candidates.append(os.path.join(root, name))
    return candidates


def main():
    st.title("ğŸ“¨ Spam Classification Demo")
    st.write("ä¸Šå‚³ CSVï¼ˆlabel,textï¼‰æˆ–å¾ dataset/uploads/data é¸æ“‡ç¾æœ‰æª”æ¡ˆï¼Œé¸æ“‡æ¨¡å‹ä¸¦è¨“ç·´ã€‚")

    with st.sidebar:
        st.header("è¨­å®š")
        model_kind = st.selectbox("æ¨¡å‹", ["svm", "lr", "nb"], index=0)
        vec_kind = st.selectbox("å‘é‡å™¨", ["tfidf", "bow"], index=0)
        ngram_max = st.slider("n-gram ä¸Šé™", min_value=1, max_value=3, value=1)
        token_pattern = st.text_input("Token Pattern (regex)", value=r"(?u)\b\w+\b")
        use_stop = st.checkbox("ä½¿ç”¨è‹±æ–‡åœç”¨è©", value=False)
        test_split = st.slider("æ¸¬è©¦é›†æ¯”ä¾‹", min_value=0.1, max_value=0.5, value=0.2, step=0.05)
        seed = st.number_input("Random Seed", min_value=0, max_value=9999, value=42)
        no_header = st.checkbox("ç„¡è¡¨é ­è³‡æ–™ï¼ˆPackt CSVï¼‰", value=True)

    source = st.radio("è³‡æ–™ä¾†æº", ["ä¸Šå‚³æª”æ¡ˆ", "å¾ç¾æœ‰æª”æ¡ˆé¸æ“‡"], horizontal=True)

    df: Optional[pd.DataFrame] = None
    if source == "ä¸Šå‚³æª”æ¡ˆ":
        uploaded = st.file_uploader("ä¸Šå‚³ CSV æª” (label,text)", type=["csv"])
        save_upload = st.checkbox("å°‡ä¸Šå‚³æª”ä¿å­˜è‡³ uploads/ ä¾›ä»–äººä½¿ç”¨", value=True)
        if uploaded is not None:
            file_bytes = uploaded.read()
            df = parse_csv(file_bytes, no_header=no_header)
            st.success(f"è³‡æ–™å·²è¼‰å…¥ï¼š{len(df)} ç­†ï¼ˆä¸Šå‚³ï¼‰")
            st.dataframe(df.head())
            if save_upload:
                os.makedirs("uploads", exist_ok=True)
                out_path = os.path.join("uploads", uploaded.name)
                with open(out_path, "wb") as w:
                    w.write(file_bytes)
                st.write(f"å·²ä¿å­˜åˆ° {out_path}")
    else:
        options = list_existing_csvs()
        if options:
            sel = st.selectbox("é¸æ“‡ç¾æœ‰ CSV æª”æ¡ˆ", options)
            if sel:
                with open(sel, "rb") as f:
                    df = parse_csv(f.read(), no_header=no_header)
                st.success(f"è³‡æ–™å·²è¼‰å…¥ï¼š{len(df)} ç­†ï¼ˆ{sel}ï¼‰")
                st.dataframe(df.head())
        else:
            st.info("å°šæœªç™¼ç¾å¯ç”¨ CSVï¼Œè«‹å…ˆæ”¾å…¥ dataset/ æˆ– data/ï¼Œæˆ–æ”¹ç”¨ä¸Šå‚³æ¨¡å¼ã€‚")

    # è‡ªå‹•è¨“ç·´ï¼šä¸€æ—¦æœ‰ df å°±ç›´æ¥è¨“ç·´èˆ‡é¡¯ç¤ºçµæœ
    if df is not None:
        with st.spinner("è¨“ç·´ä¸­..."):
            from sklearn.metrics import confusion_matrix as _confusion_matrix
            from sklearn.metrics import roc_curve as _roc_curve, auc as _auc, precision_recall_curve as _prc
            vec = build_vectorizer(vec_kind, (1, ngram_max), token_pattern, ("english" if use_stop else None))
            pipe = Pipeline([("vec", vec), ("model", get_model(model_kind))])
            X = df["text"]; y = df["label"]
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_split, random_state=seed, stratify=y)
            pipe.fit(X_train, y_train)
            y_pred = pipe.predict(X_test)
            if hasattr(pipe.named_steps["model"], "decision_function"):
                y_score = pipe.decision_function(X_test)
            elif hasattr(pipe.named_steps["model"], "predict_proba"):
                y_score = pipe.predict_proba(X_test)[:, 1]
            else:
                y_score = y_pred
            metrics = compute_basic_metrics(y_test, y_pred)

        st.subheader("æŒ‡æ¨™")
        st.json(metrics)

        # æ··æ·†çŸ©é™£
        import matplotlib.pyplot as plt

        st.subheader("æ··æ·†çŸ©é™£")
        cm = _confusion_matrix(y_test, y_pred)
        fig_cm, ax_cm = plt.subplots(figsize=(4, 4))
        im = ax_cm.imshow(cm, cmap="Blues")
        ax_cm.figure.colorbar(im, ax=ax_cm)
        ax_cm.set(xlabel="Predicted", ylabel="True")
        thresh = cm.max() / 2.0
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                ax_cm.text(j, i, int(cm[i, j]), ha="center", va="center", color="white" if cm[i, j] > thresh else "black")
        st.pyplot(fig_cm, use_container_width=True)
        plt.close(fig_cm)

        # ROC / PR
        try:
            y_score_arr = np.asarray(y_score)
            if y_score_arr.ndim == 1:
                st.subheader("ROC / PR")
                fpr, tpr, _ = _roc_curve(y_test, y_score_arr)
                roc_auc = _auc(fpr, tpr)
                fig1, ax1 = plt.subplots()
                ax1.plot(fpr, tpr, label=f"ROC AUC={roc_auc:.3f}")
                ax1.plot([0, 1], [0, 1], "k--", alpha=0.3)
                ax1.legend()
                st.pyplot(fig1, use_container_width=True)
                plt.close(fig1)

                p, r, _ = _prc(y_test, y_score_arr)
                ap = _auc(r, p)
                fig2, ax2 = plt.subplots()
                ax2.plot(r, p, label=f"AP={ap:.3f}")
                ax2.legend()
                st.pyplot(fig2, use_container_width=True)
                plt.close(fig2)

                # Threshold sweep + interactive threshold
                st.subheader("Threshold Sweep")
                df_thr = threshold_sweep(y_test, y_score_arr)
                st.line_chart(df_thr.set_index("threshold")[["precision", "recall", "f1"]])

                thr_min = float(df_thr["threshold"].min())
                thr_max = float(df_thr["threshold"].max())
                thr_default = float(np.clip(np.median(y_score_arr), thr_min, thr_max))
                step = max((thr_max - thr_min) / 100.0, 0.001)
                thr = st.slider("Decision threshold", min_value=thr_min, max_value=thr_max, value=thr_default, step=step)

                y_pred_thr = (y_score_arr >= thr).astype(int)
                st.caption("åŸºæ–¼ç›®å‰é–€æª»çš„æŒ‡æ¨™èˆ‡æ··æ·†çŸ©é™£ï¼š")
                metrics_thr = compute_basic_metrics(y_test, y_pred_thr)
                st.json(metrics_thr)

                cm2 = _confusion_matrix(y_test, y_pred_thr)
                fig_cm2, ax_cm2 = plt.subplots(figsize=(4, 4))
                im2 = ax_cm2.imshow(cm2, cmap="Purples")
                ax_cm2.figure.colorbar(im2, ax=ax_cm2)
                ax_cm2.set(xlabel="Predicted", ylabel="True", title=f"Threshold = {thr:.3f}")
                t2 = cm2.max() / 2.0
                for i in range(cm2.shape[0]):
                    for j in range(cm2.shape[1]):
                        ax_cm2.text(j, i, int(cm2[i, j]), ha="center", va="center", color="white" if cm2[i, j] > t2 else "black")
                st.pyplot(fig_cm2, use_container_width=True)
                plt.close(fig_cm2)
        except Exception:
            pass

        # Top Tokens by Class
        try:
            st.subheader("Top Tokens by Class")
            top_n = st.slider("Top-N tokens", min_value=5, max_value=100, value=30, step=5)
            from sklearn.feature_extraction.text import CountVectorizer as _CountVectorizer

            cnt_vec = _CountVectorizer(
                ngram_range=(1, ngram_max),
                token_pattern=token_pattern,
                stop_words=("english" if use_stop else None),
                lowercase=False,
            )
            X_all = cnt_vec.fit_transform(df["text"])  # ä½¿ç”¨ç›®å‰è¼‰å…¥è³‡æ–™
            vocab = np.array(cnt_vec.get_feature_names_out())
            labels_arr = df["label"].to_numpy()
            ham_mask = labels_arr == 0
            spam_mask = labels_arr == 1
            ham_counts = np.asarray(X_all[ham_mask].sum(axis=0)).ravel() if ham_mask.any() else np.zeros(X_all.shape[1])
            spam_counts = np.asarray(X_all[spam_mask].sum(axis=0)).ravel() if spam_mask.any() else np.zeros(X_all.shape[1])

            def _top_tokens(counts: np.ndarray, k: int):
                if counts.sum() == 0:
                    return pd.DataFrame({"token": [], "frequency": []})
                idx = np.argsort(counts)[::-1][:k]
                return pd.DataFrame({"token": vocab[idx], "frequency": counts[idx]})

            ham_top = _top_tokens(ham_counts, top_n)
            spam_top = _top_tokens(spam_counts, top_n)
            col1, col2 = st.columns(2)
            with col1:
                st.caption("Class: ham")
                if len(ham_top) == 0:
                    st.info("ç„¡è³‡æ–™å¯é¡¯ç¤ºï¼ˆham é¡åˆ¥ç„¡æ¨£æœ¬æˆ–å…¨ç‚ºåœç”¨è©ï¼‰")
                else:
                    st.bar_chart(ham_top.set_index("token")[ ["frequency"] ])
            with col2:
                st.caption("Class: spam")
                if len(spam_top) == 0:
                    st.info("ç„¡è³‡æ–™å¯é¡¯ç¤ºï¼ˆspam é¡åˆ¥ç„¡æ¨£æœ¬æˆ–å…¨ç‚ºåœç”¨è©ï¼‰")
                else:
                    st.bar_chart(spam_top.set_index("token")[ ["frequency"] ])
            st.caption("é »ç‡è»¸ç‚ºè©²é¡åˆ¥å…§ token å‡ºç¾æ¬¡æ•¸ï¼›token å·²ç¶“éå‰è™•ç†ï¼ˆå°å¯«èˆ‡æ¸…ç†ï¼‰ã€‚")
        except Exception:
            pass

        st.subheader("å³æ™‚é æ¸¬")
        text = st.text_input("è¼¸å…¥å¥å­é€²è¡Œé æ¸¬")
        if text:
            pred = int(pipe.predict([text])[0])
            st.write(f"é æ¸¬æ¨™ç±¤ï¼š{'spam' if pred == 1 else 'ham'}")


if __name__ == "__main__":
    main()

